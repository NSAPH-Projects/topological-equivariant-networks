#@package _global_

model:
  # defaults, other params changed by baseline/ config files
  _target_: etnn.models.ETNN
  num_out: 1
  adjacencies: ["0_0", "0_1", "1_1", "1_2", "2_2"]
  num_hidden: 32
  num_layers: 4
  num_readout_layers: 1

loader:
  _target_: torch.utils.data.DataLoader 
  num_workers: 0
  batch_size: 1
  persistent_workers: false
  pin_memory: true

collate_fn:
  _target_: etnn.combinatorial_complexes.CombinatorialComplexCollater

dataset:
  _target_: etnn.pm25.utils.SpatialCC
  root: ./data
  force_reload: true
  pre_transform: 
    _target_: etnn.pm25.utils.standardize_cc
    _partial_: true
  transform:
    _target_: etnn.pm25.utils.create_mask
    seed: ${seed}   
    _partial_: true

training:
  # defaults, other params cchanged by training/ config files
  clip: 1.0

wandb:
  entity: ten-harvard
  project: spatial-cc
  config:
    baseline: ${hydra:runtime.choices.baseline}
    seed: ${seed}
    lr: ${optimizer.lr}
    max_epochs: ${training.max_epochs}
    loss_fn: ${hydra:runtime.choices.loss_fn}
    clip: ${training.clip}

optimizer:
  _target_: torch.optim.Adam
  lr: 0.0003
  weight_decay: 1e-6

lr_scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 0.001
  final_div_factor: 1e3
  epochs: ${training.max_epochs}
  steps_per_epoch: 1
