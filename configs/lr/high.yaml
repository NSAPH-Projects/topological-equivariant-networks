#@package _global_

optimizer:
  _target_: torch.optim.Adam
  lr: 0.01
  weight_decay: 1e-4

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  eta_min: 0.001
  T_max: ${training.max_epochs}