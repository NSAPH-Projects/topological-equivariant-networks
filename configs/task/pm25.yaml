#@package _global_

model:
  # task defaults, can be overwritten by baseline
  _target_: etnn.models.ETNN
  num_out: 1
  adjacencies: ["0_0", "0_1", "1_1", "1_2", "2_2"]
  num_hidden: 4
  num_layers: 4
  num_readout_layers: 1
  
  jit: false

loader:
  _target_: torch.utils.data.DataLoader 
  num_workers: 0
  batch_size: 1
  persistent_workers: false
  pin_memory: true

collate_fn:
  _target_: etnn.combinatorial_complexes.CombinatorialComplexCollater

dataset:
  _target_: etnn.pm25.utils.SpatialCC
  root: ./data
  force_reload: true
  pre_transform: 
    _target_: etnn.pm25.utils.standardize_cc
    _partial_: true
  transform:
    _target_: etnn.pm25.utils.create_mask
    seed: ${seed}   
    _partial_: true

# logger:
#   - _target_: lightning.pytorch.loggers.WandbLogger
#     log_model: all
#     entity: ten-harvard
#     project: spatial-cc
wandb:
  entity: ten-harvard
  project: spatial-cc

# trainer:
#   _target_: lightning.pytorch.Trainer
#   max_epochs: 100
#   accelerator: auto
#   gradient_clip_val: 10
#   callbacks:
#     - _target_: lightning.pytorch.callbacks.LearningRateMonitor
#       logging_interval: epoch
#   num_sanity_val_steps: 0

trainer:
  max_epochs: 100
  clip: 1.0

optimizer:
  _target_: torch.optim.Adam
  lr: 0.0001
  weight_decay: 1e-6

lr_scheduler:
  _target_: torch.optim.lr_scheduler.OneCycleLR
  max_lr: 0.0003
  final_div_factor: 1e2
  epochs: ${trainer.max_epochs}
  steps_per_epoch: 1
